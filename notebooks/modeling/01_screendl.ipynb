{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScreenDL Modeling Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import typing as t\n",
    "\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "\n",
    "from cdrpy.models import screendl\n",
    "from cdrpy.data.datasets import Dataset, get_predictions\n",
    "from cdrpy.data.preprocess import normalize_responses\n",
    "from cdrpy.splits import load_split\n",
    "from cdrpy.mapper import BatchedResponseGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path(\"../../data/inputs/GDSCv2DepMap\")\n",
    "exp_path = (\n",
    "    input_dir / \"ScreenDL/FeatureCellToExpression1771MCGGenesTPMLogp1.csv\"\n",
    ")\n",
    "mol_path = input_dir / \"DrugToMorganFingerprint1024Bit.csv\"\n",
    "label_path = input_dir / \"LabelsLogIC50.csv\"\n",
    "split_path = input_dir / \"splits/mixed\"\n",
    "\n",
    "cell_enc = list(filter(None, screendl.load_cell_features(exp_path)))\n",
    "drug_enc = screendl.load_drug_features(mol_path)\n",
    "\n",
    "dataset = Dataset.from_csv(\n",
    "    label_path,\n",
    "    name=\"GDSCv2DepMap\",\n",
    "    cell_encoders=cell_enc,\n",
    "    drug_encoders=[drug_enc],\n",
    ")\n",
    "\n",
    "split = load_split(split_path, 1)\n",
    "\n",
    "train_ds = dataset.select(split.train_ids, name=\"train\")\n",
    "val_ds = dataset.select(split.val_ids, name=\"val\")\n",
    "test_ds = dataset.select(split.test_ids, name=\"test\")\n",
    "\n",
    "mini_ds = test_ds.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: infer_shape and infer_tspec methods in encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - [ ] Need to change to dict for encoders so that I can use cnv again\n",
    "# - [ ] Check the dense_weights are increasing with larger values\n",
    "# - [ ] Remove sample weighting for initial validations\n",
    "# - [ ] Update MLP block to enable saving\n",
    "# - [ ] Update metrics and evaluation (get_predictions) to include sample_weights\n",
    "# - [ ] Update sample weighting to clip really large or small zscores\n",
    "# - [ ] Update make_weights to apply the same weighting strategy to both train and val\n",
    "# - [ ] Add model.predict with the new evaluation schema\n",
    "# - [ ] Add hyparm optimization\n",
    "# - [ ] Add seed option in config (int or null)\n",
    "# - [ ] Convert cell_encoders and drug_encoders to an ordered dict of encoders\n",
    "#   -> that way, we can iterate over the ordered dict to encode things but also can\n",
    "#       extract key-value pairs\n",
    "# - [ ] Add l1 or l2 regularization https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = BatchedResponseGenerator(train_ds, 32)\n",
    "train_gen = generator.flow(\n",
    "    train_ds.cell_ids, train_ds.drug_ids, train_ds.labels, shuffle=True, seed=1771\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.metrics.R2Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "path = \"../../data/outputs/GDSCv2DepMap/ScreenDL/runs/2023-09-27_22-57-40/predictions.csv\"\n",
    "res = pd.read_csv(path)\n",
    "corrs = (\n",
    "    res.groupby([\"split\", \"drug_id\"])\n",
    "    .apply(lambda g: stats.pearsonr(g[\"y_true\"], g[\"y_pred\"])[0])\n",
    "    .groupby(\"split\")\n",
    "    .describe()\n",
    ")\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/outputs/GDSCv2DepMap/ScreenDL/runs/2023-09-27_21-40-26/predictions.csv\"\n",
    "res = pd.read_csv(path)\n",
    "corrs = (\n",
    "    res.groupby([\"split\", \"drug_id\"])\n",
    "    .apply(lambda g: stats.pearsonr(g[\"y_true\"], g[\"y_pred\"])[0])\n",
    "    .groupby(\"split\")\n",
    "    .describe()\n",
    ")\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights(ds: Dataset) -> np.ndarray:\n",
    "    \"\"\"Create continuous sample weights.\"\"\"\n",
    "    grouped = ds.obs.groupby(\"drug_id\")\n",
    "    sample_weights = []\n",
    "    for _, group in grouped:\n",
    "        y = group[\"label\"]\n",
    "        y_std = (y - y.min()) / (y.max() - y.min())\n",
    "        y_scaled = y_std * (1 - (-1)) + (-1)\n",
    "        sample_weights.extend(list(1 + abs(y_scaled)))\n",
    "    return np.asanyarray(sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "def make_dense_weights(\n",
    "    ds: Dataset, alpha: float = 0.5, epsilon: float = 1e-4\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Create continuous sample weights.\"\"\"\n",
    "    grouped = ds.obs.groupby(\"drug_id\")\n",
    "    sample_weights = []\n",
    "    for _, group in grouped:\n",
    "        Y = group[\"label\"]\n",
    "        kernel = stats.gaussian_kde(Y)\n",
    "\n",
    "        Z = kernel(Y)\n",
    "        Z_std = (Z - Z.min()) / (Z.max() - Z.min())\n",
    "\n",
    "        weights = np.clip((1 - (alpha * Z_std)), a_min=epsilon, a_max=None)\n",
    "        scaled_weights = weights / weights.mean()\n",
    "\n",
    "        sample_weights.extend(list(scaled_weights))\n",
    "\n",
    "    return np.asanyarray(sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_weights(train_ds).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = make_dense_weights(train_ds, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(foo.min())\n",
    "print(foo.max())\n",
    "print(foo.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train_ds.obs.copy()\n",
    "grouped = temp.groupby(\"drug_id\")\n",
    "\n",
    "for drug, group in grouped:\n",
    "    labels = group[\"label\"]\n",
    "    std_labels = (labels - labels.min()) / (labels.max() - labels.min())\n",
    "    scaled_labels = std_labels * (1 - (-1)) + (-1)\n",
    "    weights = 1 + abs(\n",
    "        scaled_labels\n",
    "    )  # NOTE: can multiple by some factor here if I want to increase weighting\n",
    "\n",
    "weights.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_labels = train_ds.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/outputs/GDSCv2DepMap/ScreenDL/runs/2023-09-27_16-20-18/predictions.csv\"\n",
    "res = pd.read_csv(path)\n",
    "corrs = (\n",
    "    res.groupby([\"split\", \"drug_id\"])\n",
    "    .apply(lambda g: stats.pearsonr(g[\"y_true\"], g[\"y_pred\"])[0])\n",
    "    .groupby(\"split\")\n",
    "    .describe()\n",
    ")\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdrpy.metrics import tf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.model.load_model(\n",
    "    \"../../data/outputs/GDSCv2DepMap/ScreenDL/runs/2023-09-27_16-20-18/model\",\n",
    "    custom_objects=tf_metrics.pearson,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. rerun as is with new config (12-58-22)\n",
    "# 2. pip install and rerun with adjusted decay rates (already adjusted) (13-38-20)\n",
    "# 3. revert to the best of 1 and 2 and rerun  with hallmark genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# break out the first hidden layer and move dropout to before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdrpy.metrics import tf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: init seeding has a large impact on convergence\n",
    "#  -> it might be better to increase batch size or to use a balanced sampler\n",
    "#   -> also try larger batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"../../data/outputs/GDSCv2DepMap/ScreenDL/runs/2023-09-26_17-18-16/model\",\n",
    "    custom_objects={\"pearson\": tf_metrics.pearson},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = model.get_layer(\"exp_dn_1\")\n",
    "weights = l.weights[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = list(pd.read_csv(exp_path, index_col=0).columns)\n",
    "gene_weights = pd.DataFrame(weights, index=genes)\n",
    "\n",
    "# NOTE: now cluster the genes and see if we recover and structure\n",
    "#   -> can apply some sort of topic labeling to the resulting clusters\n",
    "gene_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print([x[1] for x in train_gen][0][0])\n",
    "    train_gen.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asanyarray([x for x in train_gen][0][0][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds.encode_tf_v2().shuffle(10000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will need to encode the tensor shapes here somehow\n",
    "\n",
    "encoders = [cell_enc, drug_enc]\n",
    "get_tspec = lambda e: tf.TensorSpec(e.shape, tf.as_dtype(e.dtype), e.name)\n",
    "\n",
    "# encoders = [cell_enc, drug_enc]\n",
    "# # output_sig = ((tf.TensorSpec(shape=e.shape) for e in encoders), tf.TensorSpec()\n",
    "\n",
    "ds = tf.data.Dataset.from_generator(\n",
    "    mini_ds.encode_generator,\n",
    "    output_signature=(\n",
    "        tuple((get_tspec(e) for e in encoders)),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float32, name=\"label\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "for item in ds.take(1):\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen():\n",
    "    for index, row in test_ds.obs.iterrows():\n",
    "        cell_ids = [row[\"cell_id\"]]\n",
    "        drug_ids = [row[\"drug_id\"]]\n",
    "\n",
    "        cell_feat = [e.encode(cell_ids)[0] for e in [cell_enc]]\n",
    "        drug_feat = [e.encode(drug_ids)[0] for e in [drug_enc]]\n",
    "        features = tuple(cell_feat + drug_feat)\n",
    "\n",
    "        yield (features, row[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: add a .get method to the encoders for getting a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Now, I just need to encode as a generator\n",
    "\n",
    "mini_ds_tf = tf.data.Dataset.from_generator(\n",
    "    gen,\n",
    "    output_signature=(\n",
    "        (\n",
    "            (\n",
    "                tf.TensorSpec(shape=(1771,), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(1024,), dtype=tf.int32),\n",
    "            ),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.float32),\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "for item in mini_ds_tf.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = list(test_ds.encode_batches(32, as_numpy=True))\n",
    "gen[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_ds.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_inds = np.arange(32, 1000, 32)\n",
    "np.array_split(mini_ds.obs, split_inds)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "temp_path = \"https://raw.githubusercontent.com/JDACS4C-IMPROVE/DeepCDR/develop/data/CCLE/genomic_expression_561celllines_697genes_demap_features.csv\"\n",
    "temp = pd.read_csv(temp_path, index_col=0)\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "temp[:] = QuantileTransformer(output_distribution=\"normal\").fit_transform(temp)\n",
    "temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdrpy-tf-cpu-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
