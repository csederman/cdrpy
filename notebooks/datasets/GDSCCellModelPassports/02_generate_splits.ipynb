{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate train test splits for harmonized cell line data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 41\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"../../data/inputs/GDSCCellLine\")\n",
    "model_info = pd.read_csv(data_folder / \"MetaModelAnnotations.csv\")\n",
    "model_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_dose_response = pd.read_csv(data_folder / \"LabelDoseResponse.csv\")\n",
    "fitted_dose_response.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strict_train_validation_split(\n",
    "    train_model_ids: np.ndarray, train_tissues: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\"\"\"\n",
    "    uniq_tissues, tissue_counts = np.unique(train_tissues, return_counts=True)\n",
    "    keep_tissues = uniq_tissues[tissue_counts >= 2]\n",
    "\n",
    "    mask = np.isin(train_tissues, keep_tissues)\n",
    "    train_tissues_subset = train_tissues[mask]\n",
    "    train_model_ids_subset = train_model_ids[mask]\n",
    "\n",
    "    _, val_model_ids = train_test_split(\n",
    "        train_model_ids_subset,\n",
    "        random_state=SEED,\n",
    "        stratify=train_tissues_subset,\n",
    "        test_size=0.11,\n",
    "    )\n",
    "\n",
    "    train_model_ids = train_model_ids[\n",
    "        np.isin(train_model_ids, val_model_ids, invert=True)\n",
    "    ]\n",
    "\n",
    "    return train_model_ids, val_model_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tumor blind, drug blind, mixed, disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_split_folder = Path(\"../../data/inputs/GDSCCellLine/splits/tumor_blind\")\n",
    "strict_split_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "model_ids = model_info[\"model_id\"].to_numpy()\n",
    "tissues = model_info[\"cancer_type\"].to_numpy()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "split_iterator = skf.split(model_ids, tissues)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, 1):\n",
    "    train_tissues = tissues[train_idx]\n",
    "    train_model_ids = model_ids[train_idx]\n",
    "    test_model_ids = model_ids[test_idx]\n",
    "\n",
    "    train_model_ids, val_model_ids = strict_train_validation_split(\n",
    "        train_model_ids, train_tissues\n",
    "    )\n",
    "\n",
    "    train_obs_ids = fitted_dose_response[\n",
    "        fitted_dose_response[\"cell_id\"].isin(train_model_ids)\n",
    "    ][\"id\"].to_list()\n",
    "    val_obs_ids = fitted_dose_response[\n",
    "        fitted_dose_response[\"cell_id\"].isin(val_model_ids)\n",
    "    ][\"id\"].to_list()\n",
    "    test_obs_ids = fitted_dose_response[\n",
    "        fitted_dose_response[\"cell_id\"].isin(test_model_ids)\n",
    "    ][\"id\"].to_list()\n",
    "\n",
    "    train_path = Path(strict_split_folder / f\"train_{i}.pickle\")\n",
    "    val_path = Path(strict_split_folder / f\"val_{i}.pickle\")\n",
    "    test_path = Path(strict_split_folder / f\"test_{i}.pickle\")\n",
    "\n",
    "    with open(train_path, \"wb\") as fh:\n",
    "        pickle.dump(train_obs_ids, fh)\n",
    "\n",
    "    with open(val_path, \"wb\") as fh:\n",
    "        pickle.dump(val_obs_ids, fh)\n",
    "\n",
    "    with open(test_path, \"wb\") as fh:\n",
    "        pickle.dump(test_obs_ids, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_split_folder = Path(\"../../data/inputs/GDSCCellLine/splits/mixed\")\n",
    "mixed_split_folder.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_split_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_ids = fitted_dose_response[\"id\"].to_numpy()\n",
    "cell_ids = fitted_dose_response[\"cell_id\"].to_numpy()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "\n",
    "split_iterator = skf.split(obs_ids, cell_ids)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, 1):\n",
    "    train_cell_ids = cell_ids[train_idx]\n",
    "    train_obs_ids = obs_ids[train_idx]\n",
    "    test_obs_ids = obs_ids[test_idx]\n",
    "\n",
    "    train_obs_ids, val_obs_ids = train_test_split(\n",
    "        train_obs_ids,\n",
    "        random_state=SEED,\n",
    "        stratify=train_cell_ids,\n",
    "        test_size=0.11,\n",
    "    )\n",
    "    train_obs_ids = train_obs_ids.tolist()\n",
    "    val_obs_ids = val_obs_ids.tolist()\n",
    "    test_obs_ids = test_obs_ids.tolist()\n",
    "\n",
    "    train_path = Path(mixed_split_folder / f\"train_{i}.pickle\")\n",
    "    val_path = Path(mixed_split_folder / f\"val_{i}.pickle\")\n",
    "    test_path = Path(mixed_split_folder / f\"test_{i}.pickle\")\n",
    "\n",
    "    with open(train_path, \"wb\") as fh:\n",
    "        pickle.dump(train_obs_ids, fh)\n",
    "\n",
    "    with open(val_path, \"wb\") as fh:\n",
    "        pickle.dump(val_obs_ids, fh)\n",
    "\n",
    "    with open(test_path, \"wb\") as fh:\n",
    "        pickle.dump(test_obs_ids, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: what I need to do is to use the index as the split instead of the model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: now use kfold with the index\n",
    "# TODO:\n",
    "#   - [X] refactor split loaders (run_screendl)\n",
    "#   - [X] refactor the split file\n",
    "#   - [X] add a new method for selecting on the index\n",
    "#   - [X] regenerate the splits for strict versions\n",
    "#   - [X] generate splits for the lenient versions\n",
    "#   - [X] fix the fold_i+1 issues\n",
    "#   - [ ] rerun with the new splits for the 4-way experiment\n",
    "#   - [ ] once this is running, commit everything\n",
    "#   - [ ] When loading the dataset, I could set the index column and then use .loc\n",
    "#   - [ ] Pander schema validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just do a kfold cv across the responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdrpy-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
