{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Response Prediction Using an MLP Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from cdrpy.datasets import Dataset\n",
    "from cdrpy.data.preprocess import normalize_responses\n",
    "from cdrpy.mapper import BatchedResponseGenerator\n",
    "from cdrpy.metrics import tf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1771"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "### Loading the drug response dataset\n",
    "\n",
    "We can load a saved `cdrpy` datasest using the `Dataset.load` method. This demo dataset consists of harmonized cell line pharmacogenomic data retreived from the Genomics of Drug Sensitivity in Cancer and the Cell Model Passports data resources.\n",
    "\n",
    "In this dataset, cell lines are represented by log-transformed TPM gene expression values for 1771 cancer-relevant genes. Drugs are represented as 512 bit Morgan Fingerprints. Drug responses correspond to the natural log of the half-maximal inhibitory concentration (ln(IC50)) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Dataset.load(\"../../data/datasets/temp/demo.h5\")\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: add some data exploration -> how many features do we have\n",
    "# for expression and morgan fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "\n",
    "In precision oncology, we need to predict response in never-before-seen patients. To mimic this setting, we will use scikit-learn's `train_test_split` function to generate a train/test split in which all drug responses for a given cell line are held out for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_ids = D.cell_meta.index\n",
    "cell_groups = D.cell_meta[\"cancer_type\"]  # stratify by cancer type\n",
    "test_size = 0.1\n",
    "\n",
    "train_cell_ids, test_cell_ids = train_test_split(\n",
    "    cell_ids,\n",
    "    stratify=cell_groups,\n",
    "    test_size=test_size,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "train_cell_ids, val_cell_ids = train_test_split(\n",
    "    train_cell_ids,\n",
    "    stratify=cell_groups.loc[train_cell_ids],\n",
    "    test_size=test_size,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "train_ds = D.select_cells(train_cell_ids, name=\"train\")\n",
    "val_ds = D.select_cells(val_cell_ids, name=\"val\")\n",
    "test_ds = D.select_cells(test_cell_ids, name=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "To avoid data leakage, several preprocessing steps should be completed *after* splitting the data in train/validation/test sets.\n",
    "\n",
    "In this case, we will use scikit-learn's `StandardScaler` class to normalize gene expression values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_data = train_ds.cell_encoders[\"exp\"].data\n",
    "\n",
    "ge_scaler = StandardScaler().fit(ge_data.loc[train_cell_ids])\n",
    "ge_data[:] = ge_scaler.transform(ge_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to normalize the raw drug response observations. `cdrpy` provides the `normalize_responses` helper function for this purpose. This function accepts an optional parameter, `norm_method`. When `norm_method` is set to `grouped`, normalization is performed per drug, reducing bias in overall performance estimates by variability in removing drug-specific effective concentration range from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = normalize_responses(\n",
    "    train_ds, val_ds, test_ds, norm_method=\"grouped\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the model\n",
    "\n",
    "As an example, we will construct a simple model using three multi-layer perceptrons (MLPs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_input_dim = train_ds.cell_encoders[\"exp\"].shape[-1]\n",
    "fp_input_dim = train_ds.drug_encoders[\"mol\"].shape[-1]\n",
    "\n",
    "x_ge = ge_input = keras.Input((ge_input_dim,))\n",
    "x_ge = keras.layers.Dense(128, activation=\"relu\")(x_ge)\n",
    "x_ge = keras.layers.Dense(64, activation=\"relu\")(x_ge)\n",
    "\n",
    "x_fp = fp_input = keras.Input((fp_input_dim,))\n",
    "x_fp = keras.layers.Dense(128, activation=\"relu\")(x_fp)\n",
    "x_fp = keras.layers.Dense(64, activation=\"relu\")(x_fp)\n",
    "\n",
    "x = keras.layers.Concatenate()([x_ge, x_fp])\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "x_out = keras.layers.Dense(1, activation=\"linear\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model([ge_input, fp_input], x_out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-4),\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[tf_metrics.pearson],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = BatchedResponseGenerator(D, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator.flow_from_dataset(train_ds, shuffle=True, seed=SEED)\n",
    "val_gen = generator.flow_from_dataset(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_gen, epochs=50, validation_data=val_gen, verbose=2, callbacks=[early_stopping]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdrpy-tf-cpu-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
